---
title: "Work_with_processed"
author: "Allen Wang"
date: '2022-05-18'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
model_data = read.csv("D:/UCSB/Spring_2022/PSTAT 131/PSTAT_131_HW/HW2/PSTAT-131/Final Project/data/processed_data.csv")
model_ts = ts(model_data[,-1], frequency = 12, start = c(1990,2))
```

```{r}
unemp.ts = model_ts[,"unemploy_rate_la"]
unemp.ts_nocovid = window(unemp.ts, end = c(2019,12))
plot.ts(unemp.ts, main="Unemployment Rate in LA",ylab="Percentage")
plot(unemp.ts[-252],unemp.ts[-1],main="Scatterplot (lag=1)")
abline(lm(unemp.ts[-1] ~ unemp.ts[-252]),col=4)
```

### subset into training and testing data 
```{r}
umemrate_test = window(model_ts, start = 2018)
umemrate_train = window(model_ts, end = 2018)
```
###time-step features and lag features



## Predictive Modeling 
Our goal is to use previous data to predict the future value of the unemployment rate 







## ARIMA Mode Autoregressive Model

AutoRegressive (AR) model is one of the most popular time series model. In this model, each value is regressed to its previous observations. AR(1) is the first order autoregression meaning that the current value is based on the immediately preceding value.

(Unemployment Rate)time series with trends, or with seasonality, are not stationary — the trend and seasonality will affect the value of the time series at different times

one way to make a non-stationary time series stationary — compute the differences between consecutive observations. This is known as differencing. help stabilise the mean of a time series by removing changes in the level of a time series, and therefore eliminating (or reducing) trend and seasonality.


```{r}
Box.test(diff(unemp.ts_nocovid), lag=10, type="Ljung-Box")
```

```{r}
acf(unemp.ts_nocovid)
```
```{r}
pacf(unemp.ts_nocovid)
```

Seasonal Differencing
```{r}
#cbind("Percentage%" =unemp.ts,
      #"Monthly log" = log(unemp.ts),
      #"Annual change in log" = diff(log(unemp.ts),12)) %>%
  #autoplot(facets=TRUE) +
    #xlab("Year") + ylab("") +
    #ggtitle("LA Unemployment Rate")
```
```{r}
#unemp.ts %>% diff() %>% ur.kpss() %>% summary()
```

```{r}
#ndiffs(unemp.ts_nocovid)
```
```{r}
#nsdiffs(unemp.ts_nocovid)
```


```{r}
# Simulate AutoRegressive model with 0.5 slope
AR_1 <- arima.sim(model = list(ar = 0.5), n = 200)
# Simulate AutoRegressive model with 0.8 slope
#
AR_2 <- arima.sim(model = list(ar = 0.9), n = 200)
plot.ts(cbind(AR_1 , AR_2), main="AR Model Simulated Data", col = "blue")
```
```{r}

```



```{r}
cor(unemp.ts[-252],unemp.ts[-1])
```
ACF function computes (and by default plots) estimates of the autocovariance or autocorrelation function for different time lags



The x-axis donates the time lag, while the y-axis displays the estimated autocorrelation. Looking at this data, we can say that each observation is positively related to its recent past observations. However, the correlation decreases as the lag increases.

### White Noise
A white noise process is a random process of random variables that are uncorrelated, have mean zero, and a finite variance. In other words, a series is called white noise if it is purely random in nature. Random noise is donated by εt.

Plots of white noise series exhibit a very erratic, jumpy, unpredictable behavior. Since the εt are uncorrelated, previous values do not help us to forecast future values. White noise series themselves are quite uninteresting from a forecasting standpoint (they are not linearly forecastable), but they form the building blocks for more general models.

Differencing can help stabilise the mean of a time series by removing changes in the level of a time series, and therefore eliminating (or reducing) trend and seasonality. As well as looking at the time plot of the data, the ACF plot is also useful for identifying non-stationary time series.

```{r}
library(modeltime)
library(tidymodels)
library(tidyverse)
library(timetk)
library(lubridate)
```


```{r}
data = model_data %>%
  select(DATE, unemploy_rate_la) %>%
  mutate(DATE, DATE = as.Date.character(DATE))

data = data[1:320,]

data %>%plot_time_series(DATE, unemploy_rate_la)
```

```{r}
splits <- initial_time_split(data, prop = 0.9)
```

```{r}
# Auto ARIMA
model_fit_arima_no_boost <- arima_reg() %>%
    set_engine(engine = "auto_arima") %>%
    fit(unemploy_rate_la ~ DATE, data = training(splits))
```
```{r}
# Boosted Auto ARIMA
model_fit_arima_boosted <- arima_boost(
    min_n = 2,
    learn_rate = 0.015
) %>%
    set_engine(engine = "auto_arima_xgboost") %>%
    fit(unemploy_rate_la ~ DATE + as.numeric(DATE) + factor(month(DATE, label = TRUE), ordered = F),
        data = training(splits))

```
```{r}
# Exponential Smoothing
model_fit_ets <- exp_smoothing() %>%
    set_engine(engine = "ets") %>%
    fit(unemploy_rate_la ~ DATE, data = training(splits))
```
```{r}
#Prophet 
model_fit_prophet <- prophet_reg() %>%
    set_engine(engine = "prophet") %>%
    fit(unemploy_rate_la ~ DATE, data = training(splits))
```

```{r}
# Linear Regression
model_fit_lm <- linear_reg() %>%
    set_engine("lm") %>%
    fit(unemploy_rate_la ~ as.numeric(DATE) + factor(month(DATE, label = TRUE), ordered = FALSE),
        data = training(splits))
```

```{r}
library(earth)
# Multivariate Adaptive Regression Spline model 
model_spec_mars <- mars(mode = "regression") %>%
    set_engine("earth") 

recipe_spec <- recipe(unemploy_rate_la ~ DATE, data = training(splits)) %>%
    step_date(DATE, features = "month", ordinal = FALSE) %>%
    step_mutate(date_num = as.numeric(DATE)) %>%
    step_normalize(date_num) %>%
    step_rm(DATE)
  
wflw_fit_mars <- workflow() %>%
    add_recipe(recipe_spec) %>%
    add_model(model_spec_mars) %>%
    fit(training(splits))
```

Add fitted models to a Model Table
```{r}
models_tbl <- modeltime_table(
    model_fit_arima_no_boost,
    model_fit_arima_boosted,
    model_fit_ets,
    model_fit_prophet,
    model_fit_lm,
    wflw_fit_mars
)

models_tbl
```
Calibrate the model to a testing set 

```{r}
calibration_tbl <- models_tbl %>%
    modeltime_calibrate(new_data = testing(splits))
calibration_tbl
```
```{r}
calibration_tbl %>%
    modeltime_forecast(
        new_data    = testing(splits),
        actual_data = data
    ) %>%
    plot_modeltime_forecast(
      .legend_max_width = 25
    )
```

```{r}
calibration_tbl %>%
    modeltime_accuracy() %>%
    table_modeltime_accuracy(
        .interactive = FALSE
    )
```


## Random Forest



### Deep Learning





